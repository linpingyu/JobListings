{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages for scraping\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import time\n",
    "import getpass\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "#load packages for data manipulation and storage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from random import randint\n",
    "from random import uniform\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(fname, df, database_filename, if_exists='append'):\n",
    "    engine = create_engine('sqlite:///'+ database_filename)\n",
    "    df.to_sql(fname, engine, index=False, if_exists=if_exists)\n",
    "    \n",
    "def load_data(fname, database_filename):\n",
    "    engine = create_engine('sqlite:///' + database_filename)\n",
    "    df = pd.read_sql_table(fname, con=engine)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store user name and password\n",
    "session_key = input('username: ')\n",
    "session_password = getpass.getpass('password: ')\n",
    "n = input('number of jobs: ')\n",
    "\n",
    "#initialize starting url\n",
    "start_url = 'https://www.linkedin.com'\n",
    "\n",
    "#initialize driver using headless Firefox\n",
    "options = Options()\n",
    "# options.add_argument('--headless')\n",
    "driver = webdriver.Firefox(firefox_options = options)\n",
    "driver.get(start_url)\n",
    "\n",
    "#passing username and password to login\n",
    "username = driver.find_element_by_name('session_key')\n",
    "username.send_keys(session_key)\n",
    "time.sleep(uniform(1,3))\n",
    "password = driver.find_element_by_name('session_password')\n",
    "password.send_keys(session_password)\n",
    "time.sleep(uniform(1,3))\n",
    "\n",
    "driver.find_element_by_id('login-submit').click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_id('jobs-nav-item').click()\n",
    "position_name = 'data analyst'\n",
    "#fill in position name and search\n",
    "time.sleep(uniform(1,3))\n",
    "search = driver.find_element(By.XPATH,\"//input[@placeholder='Search jobs']\")\n",
    "search.send_keys(position_name)\n",
    "time.sleep(uniform(1,3))\n",
    "driver.find_element_by_class_name('jobs-search-box__submit-button.button-secondary-large-inverse').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger = driver.find_elements_by_xpath(\"//*[contains(@id,'-trigger')]\")\n",
    "for t in trigger:\n",
    "    try:\n",
    "        t.click()\n",
    "    except:\n",
    "        time.sleep(randint(1,3))\n",
    "        next\n",
    "time.sleep(randint(1,4))\n",
    "classic_view = driver.find_elements_by_xpath(\"//button[contains(@class,'dropdown')]\")[5]\n",
    "time.sleep(randint(1,2))\n",
    "classic_view.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize a new set for storing job url from each page\n",
    "job_url = defaultdict(list)\n",
    "\n",
    "#initialize a new list for avoiding duplicated job entries\n",
    "url_test = []\n",
    "\n",
    "#wait for page loading\n",
    "try:\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located(\n",
    "            (By.XPATH, \n",
    "                \"//a[@data-control-name='A_jobssearch_job_result_click']\")))\n",
    "finally:\n",
    "    #execute loop to collect job urls from each page\n",
    "    while len(job_url['urls']) < int(n):\n",
    "        #scrolling down the page to load all data\n",
    "        driver.execute_script(\"window.scroll(0, 1080);\")\n",
    "        time.sleep(randint(1,3))\n",
    "        driver.execute_script(\"window.scroll(1080, 2160);\")\n",
    "        time.sleep(randint(1,5))\n",
    "        driver.execute_script(\"window.scroll(2160, 3240);\")\n",
    "        time.sleep(randint(1,4))\n",
    "        driver.execute_script(\"window.scroll(3240, 4320);\")\n",
    "        time.sleep(randint(1,5))\n",
    "\n",
    "        #get page source and parse by BeautifulSoup\n",
    "        source = driver.page_source\n",
    "        bsObj = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "        # debug # print(bsObj.find('a',{'data-control-name':'A_jobssearch_job_result_click'}))\n",
    "        for url in bsObj.find_all('a',{'data-control-name':'A_jobssearch_job_result_click'}):\n",
    "            #debug # print(url.attrs['href'])\n",
    "            if url.attrs['href'][0:19] not in url_test:\n",
    "                job_url['urls'].append(url.attrs['href'])\n",
    "                url_test.append(url.attrs['href'][0:19])\n",
    "            else:\n",
    "                next\n",
    "\n",
    "        #monitor the number of urls collected\n",
    "        print(f\"Number of urls collected: {len(job_url['urls'])}\")\n",
    "        url_df = pd.DataFrame.from_dict(job_url)\n",
    "        save_data('urls_DA', url_df, 'LinkedinJob', if_exists='replace')\n",
    "\n",
    "        #navigate to the next page\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, \"//button[@class = 'next']\").click()\n",
    "            time.sleep(randint(1,5))\n",
    "\n",
    "        except:\n",
    "            break\n",
    "    print(f\"Number of urls collected: {len(job_url['urls'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = load_data('urls_DA', 'LinkedinJob')\n",
    "urls = urls.urls.values.tolist()\n",
    "print(f'Number of urls: {len(urls)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = defaultdict(list)\n",
    "error_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in tqdm(urls[526:]):\n",
    "    try:\n",
    "        driver.get(start_url+url)\n",
    "        time.sleep(randint(5,8))\n",
    "        see_more = driver.find_elements_by_xpath(\"//button[contains(@class, 'artdeco-button')]\")\n",
    "        for sm in see_more:\n",
    "            if sm.text.lower() == 'see more':\n",
    "                sm.click()\n",
    "        time.sleep(randint(1,5))\n",
    "        source = driver.page_source\n",
    "        bsObj = BeautifulSoup(source, 'lxml')\n",
    "    except:\n",
    "        print('Try again')\n",
    "        try:\n",
    "            see_more = driver.find_elements_by_xpath(\"//button[contains(@class, 'artdeco-button')]\")\n",
    "            for sm in see_more:\n",
    "                if sm.text.lower() == 'see more':\n",
    "                    sm.click()\n",
    "            time.sleep(randint(1,5))\n",
    "            source = driver.page_source\n",
    "            bsObj = BeautifulSoup(source, 'lxml')\n",
    "        except:\n",
    "            print(f'Error: {url}')\n",
    "            error_list.append(url)\n",
    "            \n",
    "\n",
    "        # Get job title\n",
    "    try:\n",
    "        title = bsObj.find('h1').get_text()\n",
    "    #     if title:\n",
    "    #         print(f'Title: {title}')\n",
    "        df_dict['title'].append(title)\n",
    "    except:\n",
    "        df_dict['title'].append(np.nan)\n",
    "        print(f'Cannot get title {url}')\n",
    "\n",
    "        # Get company name and location\n",
    "    try:\n",
    "        company_info = re.sub(' {2,}', '', bsObj.find('h3').get_text()).strip('\\n').split('\\n')\n",
    "        company = company_info[1]\n",
    "    #     if company:\n",
    "    #         print(f'Company: {company}')\n",
    "        df_dict['company'].append(company)\n",
    "\n",
    "        location = company_info[-1]\n",
    "    #     if location:\n",
    "    #         print(f'Location: {location}')\n",
    "        df_dict['location'].append(location)\n",
    "    except:\n",
    "        df_dict['company'].append(np.nan)\n",
    "        df_dict['location'].append(np.nan)\n",
    "        print(f'Cannot get company or location {url}')\n",
    "\n",
    "    try:\n",
    "        # Get job description\n",
    "        description = bsObj.find('div', {'id':'job-details'}).get_text().strip()\n",
    "        df_dict['description'].append(description)\n",
    "    except:\n",
    "        df_dict['description'].append(np.nan)\n",
    "        print(f'Cannot get description {url}')\n",
    "\n",
    "    try:\n",
    "        # Get seniority\n",
    "        seniority = bsObj.find('div',{'class':'jobs-description-details'}).find('p', attrs={'class':'jobs-box__body js-formatted-exp-body'})\n",
    "    #     if seniority:\n",
    "    #         print(f'Seniority: {seniority}')\n",
    "        if seniority is None:\n",
    "            df_dict['seniority'].append(np.nan)\n",
    "        else:\n",
    "            df_dict['seniority'].append(seniority.get_text())\n",
    "    except:\n",
    "        df_dict['seniority'].append(np.nan)\n",
    "        print(f'Cannot get seniority {url}')\n",
    "\n",
    "    try:\n",
    "        # Get employment type\n",
    "        employment_type = bsObj.find('div',{'class':'jobs-description-details'}).find('p', attrs={'class':'jobs-box__body js-formatted-employment-status-body'})\n",
    "    #     if employment_type:\n",
    "    #         print(f'Employment Type: {employment_type}')\n",
    "        if employment_type is None:\n",
    "            df_dict['employment_type'].append(np.nan)\n",
    "        else:\n",
    "            df_dict['employment_type'].append(employment_type.get_text())\n",
    "    except:\n",
    "        df_dict['employment_type'].append(np.nan)\n",
    "        print(f'Cannot get employent type {url}')\n",
    "\n",
    "    try:\n",
    "        # Get company industry\n",
    "        industry = bsObj.find('div',{'class':'jobs-description-details'}).find_all('ul')[0].get_text().strip().split('\\n')\n",
    "        industry = '/'.join(industry)\n",
    "        df_dict['industry'].append(industry)\n",
    "    except:\n",
    "        df_dict['industry'].append(np.nan)\n",
    "        print(f'Cannot get industry {url}')\n",
    "\n",
    "    try:\n",
    "        # Get job function\n",
    "        function = bsObj.find('div',{'class':'jobs-description-details'}).find_all('ul')[1].get_text().strip().split('\\n')\n",
    "        function = '/'.join(function)\n",
    "        df_dict['functions'].append(function)\n",
    "    except:\n",
    "        df_dict['functions'].append(np.nan)\n",
    "        print(f'Cannot get function {url}')\n",
    "    try:\n",
    "        # Get skills\n",
    "        skill = bsObj.find('div', {'class':'jobs-box__group'}).get_text().strip().replace('No match', '').split('\\n')[3:]\n",
    "        skill = [x for x in skill if x != '']\n",
    "        skill = '/'.join(skill)\n",
    "        df_dict['skills'].append(skill)\n",
    "    except:\n",
    "        df_dict['skills'].append(np.nan)\n",
    "        print(f'Cannot get skills {url}')\n",
    "\n",
    "    time.sleep(randint(1,5))\n",
    "    if len(df_dict['title']) % 10 == 0:\n",
    "        df = pd.DataFrame.from_dict(df_dict)\n",
    "        #initial parsing for text data\n",
    "        #df.company = df.company.apply(lambda x: re.sub('[\\n]*','',re.sub('^[ ]*','', x)))\n",
    "        #df.location = df.location.apply(lambda x: re.sub('\\n[ ]*', '', re.sub('^(\\n.*\\n[ ]*)','', x)))\n",
    "        #df.industry = df.industry.apply(lambda x: re.sub('^,','', re.sub('\\n', ',', x)))\n",
    "        #df.description = df.description.apply(lambda x: re.sub('( [ ]+)', '', x).strip())\n",
    "        #df.functions = df.functions.apply(lambda x: re.sub('\\n', '', x))\n",
    "\n",
    "        save_data('listingDataAnalyst', df, 'LinkedinJob')\n",
    "        df_dict = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(df_dict)\n",
    "save_data('listingDataAnalyst', df, 'LinkedinJob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.linkedin.com/jobs/view/989513065/?eBP=JOB_SEARCH_ORGANIC&refId=9671756e-77b9-4c9e-833d-7859f564371c&trk=d_flagship3_search_srp_jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
